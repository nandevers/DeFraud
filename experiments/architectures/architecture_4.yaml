nn:
  layers:
  - activation: tanh
    type: dense
    units: 64
  - type: batch_normalization
  - activation: tanh
    type: dense
    units: 64
  - type: batch_normalization
  - activation: tanh
    type: dense
    units: 64
  - type: batch_normalization
  - activation: tanh
    type: dense
    units: 64
  - type: batch_normalization
  - activation: tanh
    type: dense
    units: 64
  - type: batch_normalization
  learning_rate: 0.001
  loss: mse
  optimizer: adam
  optimizer_params:
    momentum: 0.9
tunning:
  cross_val: 10
  early_stopping:
    monitor: val_loss
    patience: 10

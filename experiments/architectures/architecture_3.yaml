nn:
  layers:
  - activation: relu
    type: batch_normalization
    units: 24
  - activation: relu
    type: batch_normalization
    units: 24
  - activation: relu
    type: batch_normalization
    units: 24
  - activation: relu
    type: batch_normalization
    units: 24
  - activation: relu
    type: batch_normalization
    units: 24
  learning_rate: 0.01
  loss: mse
  optimizer: sgd
  optimizer_params:
    beta_1: 0.9
    beta_2: 0.999
tunning:
  cross_val: 10
  early_stopping:
    monitor: val_loss
    patience: 10

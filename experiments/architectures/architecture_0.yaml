nn:
  layers:
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  - activation: relu
    type: batch_normalization
    units: 128
  learning_rate: 0.1
  loss: binary_crossentropy
  optimizer: adam
  optimizer_params:
    beta_1: 0.9
    beta_2: 0.999
tunning:
  cross_val: null
  early_stopping:
    monitor: val_acc
    patience: 10
